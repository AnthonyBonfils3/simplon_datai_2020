{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of the logistic regression updates\n",
    "\n",
    "Given the observations $X= \\{X_1,...,X_n\\}$ and the labels (or classes) $Y=\\{y_1,...,y_n \\}$ we suppose that each observation $y_i$ is identically and indepently distributed (i.i.d.) from a Bernoulli distribution with a probability of success given by\n",
    "\n",
    "$$P(y_i=1 | \\alpha, \\beta, X_i) = s(\\alpha X_i^T+\\beta) =  \\frac{1}{1+e^{-\\alpha X_i^T+\\beta}} = s_i$$\n",
    "\n",
    "The probability mass function of an observation  (ie the probability of a success OR failure) is then given by \n",
    "\n",
    "$$P(y_i | \\alpha, \\beta, X_i) = (s_i)^{y_i}(1-s_i)^{(1-y_i)}$$\n",
    "\n",
    "The likelihood of the data given the model is given by\n",
    "\n",
    "$$P(Y| \\alpha, \\beta, X) = P(y_1,...,y_n |\\alpha, \\beta, X )$$\n",
    "\n",
    "Because of the hypothesis of  i.i.d, we can rewrite the likelihood as follows\n",
    "\n",
    "$$P(Y| \\alpha, \\beta, X) = \\prod_{i=1}^N P(y_i |\\alpha, \\beta, X_i )$$\n",
    "\n",
    "What we are looking for is to **maximize** the likellood under the model parameter $\\alpha$ and $\\beta$. Because the logarithm function is stritcly monoton, is equivalent to maximize the log-likelihood, that we write as follows\n",
    "\n",
    "$$\\log P(Y| \\alpha, \\beta, X) = \\sum_{i=1}^N \\log P(y_i |\\alpha, \\beta, X_i )$$\n",
    "$$ =...$$\n",
    "$$ = \\sum_{i=1}^N y_i\\log s_i + (1-y_i)\\log (1 - s_i) $$\n",
    "$$= J(\\alpha, \\beta)$$\n",
    "\n",
    "We call the function $ J(\\alpha, \\beta)$ the cost function , and the **optimization** problem consists of maximizing this cost function quantity with respect to $\\alpha$ and $\\beta$. Thus the objectives can be written as follows\n",
    "\n",
    "$$\\underset{\\alpha, \\beta}{\\mathrm{argmax}}\\ J(\\alpha, \\beta)$$\n",
    "\n",
    "As this equation can be solved easily, we can use the gradient descent method to optimize the parameters $\\alpha$ and $\\beta$. The method consists of iteratively updating the parameters  $\\alpha$ and $\\beta$ towards a local maximum of the function $J(\\alpha, \\beta)$ by computing the gradient $\\nabla_{\\alpha, \\beta} J(\\alpha, \\beta)$\n",
    "\n",
    "The gradients can be computed as follows:\n",
    "\n",
    "* $\\nabla_\\alpha J(\\alpha, \\beta) = \\sum_{i=1}^N (y_i - s_i) X_i^T$\n",
    "* $\\nabla_\\beta J(\\alpha, \\beta) = \\sum_{i=1}^N (y_i - s_i)$\n",
    "\n",
    "And finally the update can be computed as follows\n",
    "\n",
    "$$\\alpha_{t+1} = \\alpha_t +  \\eta_1 \\nabla_\\alpha J(\\alpha, \\beta)$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t +  \\eta_2 \\nabla_\\beta J(\\alpha, \\beta)$$\n",
    "\n",
    "## Pseudo code for the gradient descent\n",
    "\n",
    "1. initialize the parameters $\\alpha$ et $\\beta$, t=0\n",
    "2. repeat until convergence \n",
    "    * compute the grandient $\\nabla_\\alpha J(\\alpha, \\beta)$ \n",
    "    * compute the grandient $\\nabla_\\alpha J(\\alpha, \\beta)$ \n",
    "    * update the parameter $\\alpha$ : $\\alpha_{t+1} = \\alpha_t +  \\eta_1 \\nabla_\\alpha J(\\alpha, \\beta)$\n",
    "    * update the parameter $\\beta$ : $\\beta_{t+1} = \\beta_t +  \\eta_2 \\nabla_\\alpha J(\\alpha, \\beta)$\n",
    "    * t = t + 1\n",
    "    \n",
    " ### Stochastic gradient descent\n",
    "    * compute the grandient $\\nabla_\\alpha J(\\alpha, \\beta, X_i, y_i)$ \n",
    "    * compute the grandient $\\nabla_\\alpha J(\\alpha, \\beta, X_i, y_i)$ \n",
    "    * repeat until convergence\n",
    "      * draw randomly a couple $(X_i, y_i)$ in all data $(X, Y)$\n",
    "      * update the parameter $\\alpha$ : $\\alpha_{t+1} = \\alpha_t +  \\eta_1 \\nabla_\\alpha J(\\alpha, \\beta,X_i, y_i)$\n",
    "      * update the parameter $\\beta$ : $\\beta_{t+1} = \\beta_t +  \\eta_2 \\nabla_\\beta J(\\alpha, \\beta,X_i, y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel de mathématiques\n",
    "\n",
    "**transposée**\n",
    "\n",
    "La transposé d'un vecteur $X=(x_1,x_2,x_3)$ s'ecrit\n",
    "\n",
    "$$X^T = \\begin{pmatrix}  x_1 \\\\ x_2 \\\\ x_3\\end{pmatrix}$$\n",
    "\n",
    "**dérivé**\n",
    "\n",
    "la dérivé d'un fonction $f(x)$ par rapport à $x$ s\"écrit \n",
    "\n",
    "$$\\frac{df(x)}{dx}$$\n",
    "\n",
    "parfois simplement écrite $f'(x)$ ou juste $f'$ si il n'y pas d'ambiguité.\n",
    "\n",
    "Pour une fonction à plusieurs variables $f(x,y)$ on peut dériver par rapport à chacune des variables. On parle de dérivée partielle et on écrit:\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x}$$ pour la dérivé par rapport à $x$ et \n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial y}$$ pour la dérivé par rapport à $y$. \n",
    "\n",
    "**gradient**\n",
    "\n",
    "Dans le cas d'un vecteur on ne parle plus de dérivé mais de gradient. Il correspond à la dérivé de chacune des entrées du vecteur. Dans le cas d'un vecteur $X=(f(x), g(x), h(x))$, son gradient s'écrit comme suit:\n",
    "\n",
    "$$\\nabla X = (f'(x), g'(x), h'(x))$$\n",
    "\n",
    "**function sigmoide**\n",
    "\n",
    "$$s(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$s'(x) = s(x)(1 - s(x))x$$\n",
    "\n",
    "**probabilité**\n",
    "\n",
    "Une variable aléatoire $X$ discrete est associé à une fonction de masse noté $P(X=x)$ souvant simplement noté $P(X)$.\n",
    "\n",
    "La valeurs de $P(X)$ pour un $x$ donnée correspond à la probabilé que $X$ prennent la valeur $x$.\n",
    "\n",
    "**probabilité conditionelle**\n",
    "\n",
    "Etant donnée deux variables aléatoires $X$ et $Y$, on note la probabilité conditionelle de $X$ sachant $Y$ \n",
    " $$P(X|Y)$$.\n",
    " \n",
    " **probabilité jointe**\n",
    " \n",
    " La probabilité jointe de deux variable aléatoire $X$ et $Y$ s'ecrit $P(X,Y)$\n",
    " \n",
    " allez plus loin:\n",
    " * règle de la somme totale\n",
    " * régle du produit\n",
    " * loi de Baille\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
